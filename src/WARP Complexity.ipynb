{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_v8bdyP72fo"
      },
      "source": [
        "# WARP Complexity Experiment\n",
        "\n",
        "We want to test if WARP can:\n",
        "\n",
        "* Understand WCA constraints.\n",
        "\n",
        "Identify growth patterns (e.g. quadratic).\n",
        "\n",
        "## Aim\n",
        "\n",
        "This experiment gives WARP the constraints of various different N, and asks it to analyze and comment on them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzxRi3P29lma"
      },
      "source": [
        "#Attempt 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KAFRoK1E72S0"
      },
      "outputs": [],
      "source": [
        "########## Prompt ##########\n",
        "\n",
        "\n",
        "prompt1 = \"\"\"You are a helpful assistant.\n",
        "User:\n",
        "Given the following examples of constraints for increasing input sizes:\n",
        "N=1: (assert (and (and (and (and (and (and (and (and (and (and  ( >=  in 0)  ( <=  in 10))  ( >  in 1))  ( >  ( -  in 1) 1))  ( >  ( -  ( -  in 1) 1) 1))  ( >  ( -  ( -  ( -  in 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1) 1) 1) 1)))\n",
        "N=12: (assert (and (and (and (and (and (and (and (and (and (and  ( >=  in 0)  ( <=  in 10))  ( >  in 1))  ( >  ( -  in 1) 1))  ( >  ( -  ( -  in 1) 1) 1))  ( >  ( -  ( -  ( -  in 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1) 1) 1) 1)))\n",
        "N=24: (assert (and (and (and (and (and (and (and (and (and (and  ( >=  in 0)  ( <=  in 10))  ( >  in 1))  ( >  ( -  in 1) 1))  ( >  ( -  ( -  in 1) 1) 1))  ( >  ( -  ( -  ( -  in 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1) 1) 1))  ( >  ( -  ( -  ( -  ( -  ( -  ( -  ( -  ( -  in 1) 1) 1) 1) 1) 1) 1) 1) 1)))\n",
        "What pattern do you observe in the number of nested operations as N increases?\n",
        "Assistant:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "Zim6_bwd88ou",
        "outputId": "84ab2e01-4ba7-4e76-fb54-134327c8cd6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 08-20 11:19:41 [utils.py:326] non-default args: {'model': 'dannkoh/warp-1.0', 'trust_remote_code': True, 'max_num_batched_tokens': 32768, 'disable_log_stats': True, 'disable_custom_all_reduce': True}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 08-20 11:19:42 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM\n",
            "WARNING 08-20 11:19:42 [__init__.py:2768] Your device 'cpu' doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
            "WARNING 08-20 11:19:42 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 08-20 11:19:42 [__init__.py:1750] Using max model len 32768\n",
            "INFO 08-20 11:19:42 [arg_utils.py:1083] Chunked prefill is not supported for ARM and POWER CPUs; disabling it for V1 backend.\n",
            "WARNING 08-20 11:19:43 [cache.py:216] Possibly too large swap space. 4.00 GiB out of the 8.00 GiB total CPU memory is allocated for the swap space.\n",
            "INFO 08-20 11:19:47 [__init__.py:241] Automatically detected platform cpu.\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:19:48 [core.py:636] Waiting for init message from front-end.\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:19:48 [core.py:74] Initializing a V1 LLM engine (v0.10.2.dev7+g5f5664b3e) with config: model='dannkoh/warp-1.0', speculative_config=None, tokenizer='dannkoh/warp-1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=dannkoh/warp-1.0, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":2,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false,\"dce\":true,\"size_asserts\":false,\"nan_asserts\":false,\"epilogue_fusion\":true},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:19:49 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:19:49 [cpu_worker.py:62] Warning: NUMA is not enabled in this build. `init_cpu_threads_env` has no effect to setup thread affinity.\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:19:49 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m WARNING 08-20 11:19:49 [cpu.py:304] Pin memory is not supported on CPU.\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:19:49 [cpu_model_runner.py:87] Starting to load model dannkoh/warp-1.0...\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:19:49 [cpu.py:100] Using Torch SDPA backend.\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:19:50 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:20<00:20, 20.43s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:45<00:00, 23.08s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:45<00:00, 22.69s/it]\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:20:36 [default_loader.py:262] Loading weights took 45.42 seconds\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:20:37 [kv_cache_utils.py:849] GPU KV cache size: 116,496 tokens\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:20:37 [kv_cache_utils.py:853] Maximum concurrency for 32,768 tokens per request: 3.56x\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:20:38 [cpu_model_runner.py:99] Warming up model for the compilation...\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:21:17 [cpu_model_runner.py:103] Warming up done.\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m INFO 08-20 11:21:17 [core.py:214] init engine (profile, create kv cache, warmup model) took 40.03 seconds\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m WARNING 08-20 11:21:22 [cache.py:216] Possibly too large swap space. 4.00 GiB out of the 8.00 GiB total CPU memory is allocated for the swap space.\n",
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m WARNING 08-20 11:21:22 [cpu.py:113] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
            "INFO 08-20 11:21:23 [llm.py:298] Supported_tasks: ['generate']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERROR 08-20 11:55:22 [core_client.py:562] Engine core proc EngineCore_0 died unexpectedly, shutting down client.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "model_id = 'dannkoh/warp-1.0'\n",
        "\n",
        "pipeline = LLM(\n",
        "    model=model_id,\n",
        "    trust_remote_code=True,\n",
        "    tensor_parallel_size=(torch.cuda.device_count() or 1),\n",
        "    dtype=\"auto\",\n",
        "    disable_custom_all_reduce=True,\n",
        "    max_num_batched_tokens=32768,\n",
        ")\n",
        "sampling_params = SamplingParams(max_tokens=8000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RTqQsLrf9W5u"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9666d57fb67c4137a3641961fb6593d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60d8e635cf9c4b4db7b1d0d766beea58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;36m(EngineCore_0 pid=59430)\u001b[0;0m WARNING 08-20 11:21:57 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/utils/__init__.py:1557\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1550\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1552\u001b[39m         warnings.warn(\n\u001b[32m   1553\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1554\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1555\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/entrypoints/llm.py:497\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    485\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    486\u001b[39m     parsed_prompts, lora_request)\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    489\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    490\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    494\u001b[39m     priority=priority,\n\u001b[32m    495\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/entrypoints/llm.py:1715\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1713\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1714\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1715\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1716\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1717\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/v1/engine/llm_engine.py:241\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[32m    244\u001b[39m iteration_stats = IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/v1/engine/core_client.py:666\u001b[39m, in \u001b[36mSyncMPClient.get_output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> EngineCoreOutputs:\n\u001b[32m    663\u001b[39m     \u001b[38;5;66;03m# If an exception arises in process_outputs_socket task,\u001b[39;00m\n\u001b[32m    664\u001b[39m     \u001b[38;5;66;03m# it is forwarded to the outputs_queue so we can raise it\u001b[39;00m\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# from this (run_output_handler) task to shut down the server.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutputs_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m    668\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/queue.py:171\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be a non-negative number\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:320\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "response = pipeline.generate(prompts=prompt1, sampling_params=sampling_params)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
