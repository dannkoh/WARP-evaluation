{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af08fce",
   "metadata": {},
   "source": [
    "# pass@k evaluation — single-model (dannkoh/warp-1.0)\n",
    "\n",
    "## Overview\n",
    "Evaluate a single local model (dannkoh/warp-1.0) on the WARP benchmark (dannkoh/WARP-benchmark, `test` split) using pass@k with m = 10 sampled candidates per problem and k = 1..10. No instruct-style prompts — use the preset instruction below. Results are saved as JSON in `src/results_dannkoh/pass@k/`.\n",
    "\n",
    "## Preset instruction (applied to every prompt)\n",
    "````text\n",
    "All per-variable constraints must be combined using a top-level (assert (and ...)) clause.\n",
    "The output must be in exact, canonical SMT-LIB format without extra commentary in the constraint string.\n",
    "Show your work in <think> </think> tags. And return the final SMT-LIB constraint string in <answer> </answer> tags.\n",
    "For example: <answer>(assert (and  ( >=  in0 97)  ( <=  in0 122)))</answer>.\n",
    "````\n",
    "\n",
    "## Configuration\n",
    "- Model: `dannkoh/warp-1.0` (single model)\n",
    "- Dataset: `dannkoh/WARP-benchmark`, split=`test`\n",
    "- Samples per problem: m = 10\n",
    "- k values: integers 1 through 10\n",
    "- Results dir: `src/results_dannkoh/pass@k/`\n",
    "- Output format: JSON (per-problem and overall summary)\n",
    "- Decoding (fixed) — documented in metadata:\n",
    "  - temperature = 0.8\n",
    "  - top_p = 0.95\n",
    "  - seed = 42 (use seed offsets per sample to produce independent draws)\n",
    "  - max_tokens = model-appropriate (e.g., 8000)\n",
    "\n",
    "## Methodology\n",
    "1. For each test item:\n",
    "   - Compose prompt with the preset instruction (no instruct-style wrapper).\n",
    "   - Generate m = 10 independent sampled candidate responses.\n",
    "   - Extract SMT-LIB answer from `<answer>...</answer>` tags.\n",
    "   - Use the repository Z3-based checker (check_logical_equivalence) to mark each candidate correct/incorrect.\n",
    "2. Compute exact pass@k per problem using the combinatorial formula (below) for k = 1..10.\n",
    "3. Aggregate pass@k across all problems by averaging per-problem pass@k values.\n",
    "4. Save per-problem records and overall summary JSON files to `src/results_dannkoh/pass@k/`.\n",
    "\n",
    "## pass@k (exact) formula\n",
    "For a problem with m samples and c correct samples among them:\n",
    "pass@k = 1 − C(m − c, k) / C(m, k)  (when k ≤ m)  \n",
    "If k > m then treat pass@k = 1 if c > 0 else 0.\n",
    "\n",
    "Compute this per problem and then average across problems for each k.\n",
    "\n",
    "## Per-problem JSON schema (saved to individual_stats.json)\n",
    "- index: int\n",
    "- tier: string\n",
    "- prompt: string\n",
    "- responses: list[string] (length m)\n",
    "- extracted: list[string] (length m)\n",
    "- correctness: list[bool] (length m)\n",
    "- reasons: list[Optional[string]] (Z3/parse failure reasons)\n",
    "- c: int (number correct)\n",
    "- pass_at_k: dict { \"1\": float, ..., \"10\": float }\n",
    "\n",
    "## Overall JSON schema (saved to overall_stats.json)\n",
    "- model: \"dannkoh/warp-1.0\"\n",
    "- dataset: \"dannkoh/WARP-benchmark\"\n",
    "- m: 10\n",
    "- k_values: [1,2,...,10]\n",
    "- decoding: {temperature, top_p, seed, ...}\n",
    "- total_problems: int\n",
    "- pass_at_k: {\"1\": float, ..., \"10\": float}\n",
    "\n",
    "Example:\n",
    "````json\n",
    "{\n",
    "  \"model\": \"dannkoh/warp-1.0\",\n",
    "  \"dataset\": \"dannkoh/WARP-benchmark\",\n",
    "  \"m\": 10,\n",
    "  \"k_values\": [1,2,3,4,5,6,7,8,9,10],\n",
    "  \"decoding\": {\"temperature\": 0.8, \"top_p\": 0.95, \"seed\": 42},\n",
    "  \"total_problems\": 1234,\n",
    "  \"pass_at_k\": {\"1\": 0.12, \"2\": 0.17, \"3\": 0.20, \"4\": 0.22, \"5\": 0.24, \"6\": 0.25, \"7\": 0.26, \"8\": 0.27, \"9\": 0.27, \"10\": 0.28}\n",
    "}\n",
    "````\n",
    "\n",
    "## Reproducibility and choices\n",
    "- Fix decoding hyperparameters and record them in the JSON header.\n",
    "- Use deterministic RNG seed base (e.g., 42) and vary by sample index (42 + i) to create independent draws while allowing reproducibility.\n",
    "- Decide whether to deduplicate identical extracted SMT-LIB strings before counting c; document and choose one approach (recommended: do not deduplicate unless justified).\n",
    "- Log any parse/extraction failures and include reasons in per-problem output.\n",
    "\n",
    "## Post-processing & visualization\n",
    "- Use the overall `pass_at_k` curve (k = 1..10) to show how performance improves with more samples.\n",
    "- Optionally break down pass@k by tier and save tiered summaries compatible with the repository aggregator.\n",
    "\n",
    "## Running summary\n",
    "- Ensure HF token and vLLM environment are configured.\n",
    "- Run the notebook/script that implements the above steps and writes JSON to `src/results_dannkoh/pass@k/`.\n",
    "- Feed results into aggregate.py (adapt as needed) for tiered summaries and percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858e91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.configs import ModelConfig\n",
    "from utils.evaluation import LLMHelper, Loader, check_logical_equivalence\n",
    "\n",
    "# Configuration (edit as needed)\n",
    "MODEL = \"dannkoh/warp-1.0\"\n",
    "DATASET = \"dannkoh/WARP-benchmark\"\n",
    "SPLIT = \"test\"\n",
    "M = 10\n",
    "K_VALUES = list(range(1, 11))\n",
    "BATCH_SIZE = 64\n",
    "RESULTS_DIR = Path(\"results_dannkoh/pass@k\")\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085965c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pass_at_k(m: int, c: int, k_values: List[int]) -> Dict[int, float]:\n",
    "    out: Dict[int, float] = {}\n",
    "    for k in k_values:\n",
    "        if c == 0:\n",
    "            out[k] = 0.0\n",
    "        elif k >= m:\n",
    "            out[k] = 1.0 if c > 0 else 0.0\n",
    "        else:\n",
    "            out[k] = 1.0 - math.comb(m - c, k) / math.comb(m, k)\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_passk(\n",
    "    model_id: str,\n",
    "    dataset_id: str = DATASET,\n",
    "    split: str = SPLIT,\n",
    "    m: int = M,\n",
    "    k_values: Optional[List[int]] = None,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    results_dir: Path = RESULTS_DIR,\n",
    ") -> Dict[str, Any]:\n",
    "    if k_values is None:\n",
    "        k_values = list(range(1, m + 1))\n",
    "\n",
    "    modelcfg = ModelConfig(\n",
    "        model_name=model_id,\n",
    "        quantization_mode=None,\n",
    "        token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "        instruct=False,\n",
    "    )\n",
    "    llm = LLMHelper(modelconfig=modelcfg)\n",
    "\n",
    "\n",
    "    ds = load_dataset(dataset_id, split=split)\n",
    "    items = []\n",
    "    for idx, ex in enumerate(ds):\n",
    "        question_field = ex.get(\"question\") or ex.get(\"prompt\") or ex.get(\"input\") or ex.get(\"problem\") or \"\"\n",
    "        prompt = Loader.apply_chat_template(prompt=question_field, instruct=False)\n",
    "        items.append({\n",
    "            \"index\": int(ex.get(\"index\", idx)),\n",
    "            \"tier\": ex.get(\"tier\", \"unknown\"),\n",
    "            \"prompt\": prompt,\n",
    "            \"truth\": (ex.get(\"answer\") or \"\").strip(),\n",
    "            \"constants\": ex.get(\"constants\", None),\n",
    "        })\n",
    "\n",
    "    records: List[Dict[str, Any]] = []\n",
    "    total = len(items)\n",
    "    if total == 0:\n",
    "        raise RuntimeError(\"No examples found in dataset/split\")\n",
    "\n",
    "    for batch_start in tqdm(range(0, total, batch_size), desc=\"Batches\"):\n",
    "        batch = items[batch_start: batch_start + batch_size]\n",
    "        prompts = [b[\"prompt\"] for b in batch]\n",
    "        responses_per_example: List[List[str]] = [[] for _ in batch]\n",
    "\n",
    "        for _ in range(m):\n",
    "            outs = llm.get_response(prompts)\n",
    "            if len(outs) != len(prompts):\n",
    "                outs = (outs + [\"\"] * len(prompts))[: len(prompts)]\n",
    "            for i, o in enumerate(outs):\n",
    "                responses_per_example[i].append(o)\n",
    "\n",
    "        for meta, responses in zip(batch, responses_per_example):\n",
    "            extracted: List[str] = []\n",
    "            correctness: List[bool] = []\n",
    "            reasons: List[Optional[str]] = []\n",
    "            for resp in responses:\n",
    "                try:\n",
    "                    ans = Loader.extract_response(resp)\n",
    "                except Exception as e:\n",
    "                    ans = \"\"\n",
    "                    extracted.append(ans)\n",
    "                    correctness.append(False)\n",
    "                    reasons.append(f\"extraction_failure: {e}\")\n",
    "                    continue\n",
    "\n",
    "                extracted.append(ans)\n",
    "                try:\n",
    "                    result = check_logical_equivalence(\n",
    "                        original_assertions=meta[\"truth\"],\n",
    "                        generated_assertions=ans,\n",
    "                        constants=meta[\"constants\"],\n",
    "                    )\n",
    "                    is_correct = bool(result.get(\"result\", False))\n",
    "                    reason = result.get(\"reason\")\n",
    "                except Exception as e:\n",
    "                    is_correct = False\n",
    "                    reason = f\"checker_exception: {e}\"\n",
    "\n",
    "                correctness.append(is_correct)\n",
    "                reasons.append(reason)\n",
    "\n",
    "            c = sum(1 for v in correctness if v)\n",
    "            pass_at_k = compute_pass_at_k(m=m, c=c, k_values=k_values)\n",
    "\n",
    "            records.append({\n",
    "                \"index\": meta[\"index\"],\n",
    "                \"tier\": meta[\"tier\"],\n",
    "                \"prompt\": meta[\"prompt\"],\n",
    "                \"responses\": responses,\n",
    "                \"extracted\": extracted,\n",
    "                \"correctness\": correctness,\n",
    "                \"reasons\": reasons,\n",
    "                \"c\": int(c),\n",
    "                \"pass_at_k\": {str(k): pass_at_k[k] for k in k_values},\n",
    "            })\n",
    "\n",
    "    overall_pass: Dict[str, float] = {str(k): 0.0 for k in k_values}\n",
    "    for rec in records:\n",
    "        for k in k_values:\n",
    "            overall_pass[str(k)] += rec[\"pass_at_k\"][str(k)]\n",
    "    if records:\n",
    "        for k in k_values:\n",
    "            overall_pass[str(k)] /= len(records)\n",
    "\n",
    "    overall = {\n",
    "        \"model\": model_id,\n",
    "        \"dataset\": dataset_id,\n",
    "        \"split\": split,\n",
    "        \"m\": m,\n",
    "        \"k_values\": [int(k) for k in k_values],\n",
    "        \"total_problems\": len(records),\n",
    "        \"pass_at_k\": overall_pass,\n",
    "    }\n",
    "\n",
    "    (results_dir / \"individual_stats.json\").write_text(json.dumps(records, indent=2))\n",
    "    (results_dir / \"overall_stats.json\").write_text(json.dumps(overall, indent=2))\n",
    "\n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d69993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 11:23:53 [__init__.py:241] Automatically detected platform cpu.\n",
      "INFO 08-19 11:23:54 [utils.py:326] non-default args: {'model': 'dannkoh/warp-1.0', 'trust_remote_code': True, 'max_model_len': 8000, 'max_num_batched_tokens': 8000, 'disable_log_stats': True, 'disable_custom_all_reduce': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-19 11:24:00 [__init__.py:711] Resolved architecture: Qwen2ForCausalLM\n",
      "WARNING 08-19 11:24:00 [__init__.py:2768] Your device 'cpu' doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
      "WARNING 08-19 11:24:00 [__init__.py:2819] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 08-19 11:24:00 [__init__.py:1750] Using max model len 8000\n",
      "WARNING 08-19 11:24:00 [cpu.py:113] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "INFO 08-19 11:24:00 [arg_utils.py:1083] Chunked prefill is not supported for ARM and POWER CPUs; disabling it for V1 backend.\n",
      "WARNING 08-19 11:24:01 [cache.py:216] Possibly too large swap space. 4.00 GiB out of the 8.00 GiB total CPU memory is allocated for the swap space.\n",
      "INFO 08-19 11:24:06 [__init__.py:241] Automatically detected platform cpu.\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:24:07 [core.py:636] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:24:07 [core.py:74] Initializing a V1 LLM engine (v0.10.2.dev7+g5f5664b3e) with config: model='dannkoh/warp-1.0', speculative_config=None, tokenizer='dannkoh/warp-1.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8000, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cpu, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=dannkoh/warp-1.0, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":2,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[\"none\"],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false,\"dce\":true,\"size_asserts\":false,\"nan_asserts\":false,\"epilogue_fusion\":true},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:24:07 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:24:08 [cpu_worker.py:62] Warning: NUMA is not enabled in this build. `init_cpu_threads_env` has no effect to setup thread affinity.\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:24:08 [parallel_state.py:1134] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m WARNING 08-19 11:24:08 [cpu.py:304] Pin memory is not supported on CPU.\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:24:08 [cpu_model_runner.py:87] Starting to load model dannkoh/warp-1.0...\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:24:08 [cpu.py:100] Using Torch SDPA backend.\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:24:08 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:21<00:21, 21.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:36<00:00, 17.41s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:36<00:00, 18.00s/it]\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:28:08 [default_loader.py:262] Loading weights took 36.06 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:28:08 [kv_cache_utils.py:849] GPU KV cache size: 116,496 tokens\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:28:08 [kv_cache_utils.py:853] Maximum concurrency for 8,000 tokens per request: 14.56x\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:28:08 [cpu_model_runner.py:99] Warming up model for the compilation...\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:29:19 [cpu_model_runner.py:103] Warming up done.\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m INFO 08-19 11:29:19 [core.py:214] init engine (profile, create kv cache, warmup model) took 70.92 seconds\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m WARNING 08-19 11:29:21 [cache.py:216] Possibly too large swap space. 4.00 GiB out of the 8.00 GiB total CPU memory is allocated for the swap space.\n",
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m WARNING 08-19 11:29:21 [cpu.py:113] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "INFO 08-19 11:29:21 [llm.py:298] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3d7a16983c4d8697b43e82cb2c55b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d756e621dda74b27808fc613e85672e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset.parquet:   0%|          | 0.00/427k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f465d5f7f0614b02a230d89fd6e6bf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cda424baefe45e195e2b3eead3267ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea3be53f7f24d64b011143782a41ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc016c8d0994682bfff87edcb4d376f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/64 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_0 pid=47360)\u001b[0;0m WARNING 08-19 11:29:30 [cudagraph_dispatcher.py:101] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m overall = \u001b[43mrun_passk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSPLIT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK_VALUES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresults_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRESULTS_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mOverall summary:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(overall, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mrun_passk\u001b[39m\u001b[34m(model_id, dataset_id, split, m, k_values, batch_size, results_dir)\u001b[39m\n\u001b[32m     55\u001b[39m responses_per_example: List[List[\u001b[38;5;28mstr\u001b[39m]] = [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     outs = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(outs) != \u001b[38;5;28mlen\u001b[39m(prompts):\n\u001b[32m     60\u001b[39m         outs = (outs + [\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(prompts))[: \u001b[38;5;28mlen\u001b[39m(prompts)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/invaR1ant-evaluation/src/utils/evaluation.py:204\u001b[39m, in \u001b[36mLLMHelper.get_response\u001b[39m\u001b[34m(self, history)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_response\u001b[39m(\u001b[38;5;28mself\u001b[39m, history: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    201\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m    Get the response from the model.\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhelper\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/invaR1ant-evaluation/src/utils/evaluation.py:172\u001b[39m, in \u001b[36m_VLLMHelper._get_responses\u001b[39m\u001b[34m(self, prompts)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_responses\u001b[39m(\u001b[38;5;28mself\u001b[39m, prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# generate all prompts in one shot\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [o.outputs[\u001b[32m0\u001b[39m].text.strip() \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/utils/__init__.py:1557\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1550\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1552\u001b[39m         warnings.warn(\n\u001b[32m   1553\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1554\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1555\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/entrypoints/llm.py:497\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, priority)\u001b[39m\n\u001b[32m    485\u001b[39m lora_request = \u001b[38;5;28mself\u001b[39m._get_modality_specific_lora_reqs(\n\u001b[32m    486\u001b[39m     parsed_prompts, lora_request)\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    489\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    490\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    494\u001b[39m     priority=priority,\n\u001b[32m    495\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine_class.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/entrypoints/llm.py:1715\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m   1713\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m   1714\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m-> \u001b[39m\u001b[32m1715\u001b[39m     step_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1716\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m   1717\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/v1/engine/llm_engine.py:241\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[32m    244\u001b[39m iteration_stats = IterationStats() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vllm/vllm/v1/engine/core_client.py:666\u001b[39m, in \u001b[36mSyncMPClient.get_output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    662\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> EngineCoreOutputs:\n\u001b[32m    663\u001b[39m     \u001b[38;5;66;03m# If an exception arises in process_outputs_socket task,\u001b[39;00m\n\u001b[32m    664\u001b[39m     \u001b[38;5;66;03m# it is forwarded to the outputs_queue so we can raise it\u001b[39;00m\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# from this (run_output_handler) task to shut down the server.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutputs_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m    668\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/queue.py:171\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m timeout < \u001b[32m0\u001b[39m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m'\u001b[39m\u001b[33m must be a non-negative number\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:320\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR 08-19 11:55:10 [core_client.py:562] Engine core proc EngineCore_0 died unexpectedly, shutting down client.\n"
     ]
    }
   ],
   "source": [
    "overall = run_passk(\n",
    "    model_id=MODEL,\n",
    "    dataset_id=DATASET,\n",
    "    split=SPLIT,\n",
    "    m=M,\n",
    "    k_values=K_VALUES,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    results_dir=RESULTS_DIR,\n",
    ")\n",
    "\n",
    "print(\"Overall summary:\")\n",
    "print(json.dumps(overall, indent=2))\n",
    "\n",
    "# plot pass@k curve\n",
    "k_vals = [int(k) for k in overall[\"k_values\"]]\n",
    "p_vals = [overall[\"pass_at_k\"][str(k)] for k in k_vals]\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(k_vals, p_vals, marker=\"o\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"pass@k\")\n",
    "plt.title(f\"pass@k — {MODEL} on {DATASET}\")\n",
    "plt.grid(True)\n",
    "plt.xticks(k_vals)\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / \"pass_at_k_curve.png\", dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
