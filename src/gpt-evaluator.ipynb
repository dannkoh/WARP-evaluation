{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124f7599",
   "metadata": {},
   "source": [
    "# GPT WARP Evaluator\n",
    "This notebook is used to evaluate the performance of different GPT models on the WARP benchmark. We utilise the OpenAI batch API to efficiently evaluate multiple models on the benchmark. User needs to specify their own OpenAI API key and feed it into the client. This notebook is still under development but is functional.\n",
    "\n",
    "This script will:\n",
    "1. Create the batch jsonl files for each model stored in the `results/batch` directory\n",
    "2. Upload the batch files to OpenAI\n",
    "3. Queue the 3 batches for processing for each model \n",
    "4. Download the results to the `results` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8628696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "import openai\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e0fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.content(\"file-HdvpqA9Pmu4SZi7R1AmMo8\").write_to_file(\"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the benchmark dataset\n",
    "dataset = load_dataset(\"dannkoh/warp-benchmark\", split=\"test\")\n",
    "dataset = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d92889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(str):\n",
    "    end = \"All per-variable constraints must be combined using a top-level (assert (and ...)) clause.\\nThe output must be in exact, canonical SMT-LIB format without extra commentary in the constraint string.\\nShow your work in <think> </think> tags. And return the final SMT-LIB constraint string in <answer> </answer> tags.\\nFor example: <answer>(assert (and  ( >=  in0 97)  ( <=  in0 122)))</answer>.\"\n",
    "    return end + \"\\n\" + str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c576428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose your models into an array.\n",
    "# models = [\"gpt-4.1-2025-04-14\", \"gpt-4.1-mini-2025-04-14\", \"gpt-4o-2024-11-20\",\"o3-mini-2025-01-31\",\"o4-mini-2025-04-16\"]\n",
    "models = [\"gpt-5-2025-08-07\"]\n",
    "\n",
    "\n",
    "endpoint = \"/v1/chat/completions\"\n",
    "\n",
    "batches = { i : [] for i in range(3)}\n",
    "\n",
    "for model in models:\n",
    "    batch = []\n",
    "    for i, row in dataset.iterrows():\n",
    "        batch.append(\n",
    "            {\n",
    "                \"custom_id\": str(i),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": model, \"messages\": [{\"role\": \"user\", \"content\": format_question(row[\"question\"])}]},\n",
    "            }\n",
    "        )\n",
    "    with open(f\"../results/batch/{model}-batch.jsonl\", \"w\") as f:\n",
    "        for item in batch:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    file = client.files.create(\n",
    "        file=open(f\"../results/batch/{model}-batch.jsonl\", \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    for trial in range(3):\n",
    "        batches[trial].append(\n",
    "            client.batches.create(\n",
    "                input_file_id=file.id,\n",
    "                endpoint=endpoint,\n",
    "                completion_window=\"24h\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def load_z3_template(constants: str, original_assertions: str, generated_assertions: str) -> str:\n",
    "    \"\"\"\n",
    "    Load the Z3 template for the experiment.\n",
    "    \"\"\"\n",
    "    return f\"\"\"; Combined SMT for checking equivalence\n",
    "; Original constants:\n",
    "{constants}\n",
    "\n",
    "; Original constraints (A):\n",
    "(push)\n",
    "{original_assertions}\n",
    "(pop)\n",
    "\n",
    "; Generated constraints (B):\n",
    "(push)\n",
    "{generated_assertions}\n",
    "(pop)\n",
    "\n",
    "; Now do two checks:\n",
    "; 1) A => B fails means we push A and then (not B)\n",
    "(push)\n",
    "{original_assertions}\n",
    "(assert (not\n",
    "{_parse_constraints(generated_assertions)}\n",
    "))\n",
    "(check-sat)\n",
    "(pop)\n",
    "\n",
    "; 2) B => A fails means we push B and then (not A)\n",
    "(push)\n",
    "{generated_assertions}\n",
    "(assert (not\n",
    "{_parse_constraints(original_assertions)}\n",
    "))\n",
    "(check-sat)\n",
    "(pop)\n",
    "\"\"\"\n",
    "\n",
    "def _parse_constraints(constraints: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse raw SMT-LIB2 constraints into a single conjunctive form.\n",
    "    \"\"\"\n",
    "    assertions = [line.strip()[8:-1] for line in constraints.splitlines() if line.startswith(\"(assert\")]\n",
    "    return f\"(and {' '.join(assertions)})\"\n",
    "\n",
    "def extract_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the response from the experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return response.split(\"<answer>\")[1].split(\"</answer>\")[0].strip().replace(\"\\n\", \"\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def evaluate_with_z3(response: str, truth: str, constants: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the response using Z3.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    smt2 = load_z3_template(constants=constants, original_assertions=truth, generated_assertions=response)\n",
    "\n",
    "    try:\n",
    "        proc = subprocess.run([\"z3\", \"-in\"], input=smt2, capture_output=True, text=True, check=False)\n",
    "        output = proc.stdout.strip()\n",
    "\n",
    "        results = [line for line in output.splitlines() if line in (\"sat\", \"unsat\", \"unknown\")]\n",
    "\n",
    "        if len(results) < 2:\n",
    "            result[\"result\"] = False\n",
    "            result[\"reason\"] = \"Could not parse results correctly.\"\n",
    "            return result\n",
    "\n",
    "        if results[0] != \"unsat\":\n",
    "            result[\"result\"] = False\n",
    "            result[\"reason\"] = \"Original does not imply generated.\"\n",
    "            return result\n",
    "\n",
    "        if results[1] != \"unsat\":\n",
    "            result[\"result\"] = False\n",
    "            result[\"reason\"] = \"Generated does not imply original.\"\n",
    "            return result\n",
    "\n",
    "        result[\"result\"] = True\n",
    "        result[\"reason\"] = \"Constraints are logically equivalent.\"\n",
    "    except Exception as e:\n",
    "        result[\"result\"] = False\n",
    "        result[\"reason\"] = f\"Error running Z3: {e}\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_completed = True\n",
    "for trial_batches in batches.values():\n",
    "    for batch in trial_batches:\n",
    "        batch_info = client.batches.retrieve(batch.id)\n",
    "        if batch_info.status != \"completed\":\n",
    "            all_completed = False\n",
    "            print(f\"Batch {batch.id} is not completed. Status: {batch_info.status}\")\n",
    "\n",
    "if all_completed:\n",
    "    print(\"All batches are completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d62f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for trial_batches in batches.values():\n",
    "    for batch in trial_batches:\n",
    "        output_dir = f\"../results/{trial_batches}\"\n",
    "        output_file_path = f\"{output_dir}/{client.files.retrieve(batch.input_file_id).filename}\"\n",
    "        client.files.content(batch.output_file_id).write_to_file(f\"{output_file_path}\")\n",
    "        model = client.files.retrieve(batch.input_file_id).filename.rsplit(\"-\",maxsplit=1)[0]\n",
    "\n",
    "        with open(f\"{output_file_path}\") as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Extract content from each response\n",
    "        df[\"content\"] = df[\"response\"].apply(\n",
    "            lambda r: (\n",
    "                r[\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "                if (r and \"body\" in r and \"choices\" in r[\"body\"] and len(r[\"body\"][\"choices\"]) > 0)\n",
    "                else None\n",
    "            )\n",
    "        )\n",
    "        df = df[[\"custom_id\", \"content\"]]\n",
    "        df[\"custom_id\"] = df[\"custom_id\"].astype(int)\n",
    "\n",
    "        # Merge with original dataset\n",
    "        merged_df = dataset.merge(df, left_index=True, right_on=\"custom_id\", how=\"left\")\n",
    "\n",
    "        # Evaluate each response\n",
    "        results = []\n",
    "        for i, row in merged_df.iterrows():\n",
    "            response = row[\"content\"]\n",
    "            truth = row[\"answer\"]\n",
    "            constants = row[\"constants\"]\n",
    "            question = row[\"question\"]\n",
    "\n",
    "            extracted = extract_response(response) if response else None\n",
    "            z3_result = (\n",
    "                evaluate_with_z3(response=extracted, truth=truth, constants=constants)\n",
    "                if extracted else {\"result\": False, \"reason\": \"Failed to extract response.\"}\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"custom_id\": row[\"custom_id\"],\n",
    "                \"question\": question,\n",
    "                \"response\": response,\n",
    "                \"extracted\": extracted,\n",
    "                \"truth\": truth,\n",
    "                \"constants\": constants,\n",
    "                \"z3_result\": z3_result[\"result\"],\n",
    "                \"reason\": z3_result[\"reason\"]\n",
    "            })\n",
    "\n",
    "        # Compute final metrics and save\n",
    "        result_summary = {\n",
    "            \"model\": model,\n",
    "            \"accuracy\": sum(1 for r in results if r[\"z3_result\"]) / len(results) if len(results) else 0,\n",
    "            \"correct\": sum(1 for r in results if r[\"z3_result\"]),\n",
    "            \"incorrect\": sum(1 for r in results if not r[\"z3_result\"]),\n",
    "            \"total\": len(results),\n",
    "            \"results\": results,\n",
    "        }\n",
    "\n",
    "\n",
    "        with open(f\"{output_dir}/{model}.json\", \"w\") as f:\n",
    "            json.dump(result_summary, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
