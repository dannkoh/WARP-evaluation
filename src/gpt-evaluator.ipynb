{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8628696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc6124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the benchmark dataset\n",
    "dataset = load_dataset(\"dannkoh/invaR1ant-benchmark\", split=\"test\")\n",
    "dataset = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6715b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d92889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(str):\n",
    "    end = \"All per-variable constraints must be combined using a top-level (assert (and ...)) clause.\\nThe output must be in exact, canonical SMT-LIB format without extra commentary in the constraint string.\\nShow your work in <think> </think> tags. And return the final SMT-LIB constraint string in <answer> </answer> tags.\\nFor example: <answer>(assert (and  ( >=  in0 97)  ( <=  in0 122)))</answer>.\"\n",
    "    return end + \"\\n\" + str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c576428",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"gpt-4.1-2025-04-14\", \"gpt-4.1-mini-2025-04-14\", \"gpt-4o-2024-11-20\",\"o3-mini-2025-01-31\",\"o4-mini-2025-04-16\"]\n",
    "endpoint = \"/v1/chat/completions\"\n",
    "\n",
    "for model in models:\n",
    "    batch = []\n",
    "    for i, row in dataset.iterrows():\n",
    "        batch.append(\n",
    "            {\n",
    "                \"custom_id\": str(i),\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\"model\": model, \"messages\": [{\"role\": \"user\", \"content\": format_question(row[\"question\"])}]},\n",
    "            }\n",
    "        )\n",
    "    # Save the batch as a jsonl file and upload it to the OpenAI API\n",
    "    with open(f\"results/batch/{model}-batch.jsonl\", \"w\") as f:\n",
    "        for item in batch:\n",
    "            json.dump(item, f)\n",
    "            f.write(\"\\n\")\n",
    "    # Upload the batch to OpenAI\n",
    "\n",
    "    client.files.create(\n",
    "        file=open(f\"results/batch/{model}-batch.jsonl\", \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8b5384",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in client.files.list():\n",
    "    if file.purpose == \"batch\":\n",
    "        client.batches.create(\n",
    "            completion_window=\"24h\",\n",
    "            endpoint=endpoint,\n",
    "            input_file_id=file.id\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37571bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in client.files.list():\n",
    "    if file.purpose == \"batch\":\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in client.batches.list():\n",
    "    if batch.created_at >= 1746474126:\n",
    "        print(datetime.datetime.fromtimestamp(batch.created_at),batch)\n",
    "\n",
    "#\n",
    "# 4o-mini trial 2 Batch(id='batch_67fe3cd614788190b108a5f4d0398928', completion_window='24h', created_at=1744714966, endpoint='/v1/chat/completions', input_file_id='file-EUgdDeKHeWtkUjsUwJxq6Y', object='batch', status='in_progress', cancelled_at=None, cancelling_at=None, completed_at=None, error_file_id=None, errors=None, expired_at=None, expires_at=1744801366, failed_at=None, finalizing_at=None, in_progress_at=1744714967, metadata=None, output_file_id=None, request_counts=BatchRequestCounts(completed=0, failed=0, total=671))\n",
    "# print(f\"{'#'* 20}\")\n",
    "# for batch in client.batches.list():\n",
    "#         print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4e96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def load_z3_template(constants: str, original_assertions: str, generated_assertions: str) -> str:\n",
    "    \"\"\"\n",
    "    Load the Z3 template for the experiment.\n",
    "    \"\"\"\n",
    "    return f\"\"\"; Combined SMT for checking equivalence\n",
    "; Original constants:\n",
    "{constants}\n",
    "\n",
    "; Original constraints (A):\n",
    "(push)\n",
    "{original_assertions}\n",
    "(pop)\n",
    "\n",
    "; Generated constraints (B):\n",
    "(push)\n",
    "{generated_assertions}\n",
    "(pop)\n",
    "\n",
    "; Now do two checks:\n",
    "; 1) A => B fails means we push A and then (not B)\n",
    "(push)\n",
    "{original_assertions}\n",
    "(assert (not\n",
    "{_parse_constraints(generated_assertions)}\n",
    "))\n",
    "(check-sat)\n",
    "(pop)\n",
    "\n",
    "; 2) B => A fails means we push B and then (not A)\n",
    "(push)\n",
    "{generated_assertions}\n",
    "(assert (not\n",
    "{_parse_constraints(original_assertions)}\n",
    "))\n",
    "(check-sat)\n",
    "(pop)\n",
    "\"\"\"\n",
    "\n",
    "def _parse_constraints(constraints: str) -> str:\n",
    "    \"\"\"\n",
    "    Parse raw SMT-LIB2 constraints into a single conjunctive form.\n",
    "    \"\"\"\n",
    "    assertions = [line.strip()[8:-1] for line in constraints.splitlines() if line.startswith(\"(assert\")]\n",
    "    return f\"(and {' '.join(assertions)})\"\n",
    "\n",
    "def extract_response(response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the response from the experiment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return response.split(\"<answer>\")[1].split(\"</answer>\")[0].strip().replace(\"\\n\", \"\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def evaluate_with_z3(response: str, truth: str, constants: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate the response using Z3.\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    smt2 = load_z3_template(constants=constants, original_assertions=truth, generated_assertions=response)\n",
    "\n",
    "    try:\n",
    "        proc = subprocess.run([\"z3\", \"-in\"], input=smt2, capture_output=True, text=True, check=False)\n",
    "        output = proc.stdout.strip()\n",
    "\n",
    "        results = [line for line in output.splitlines() if line in (\"sat\", \"unsat\", \"unknown\")]\n",
    "\n",
    "        if len(results) < 2:\n",
    "            result[\"result\"] = False\n",
    "            result[\"reason\"] = \"Could not parse results correctly.\"\n",
    "            return result\n",
    "\n",
    "        if results[0] != \"unsat\":\n",
    "            result[\"result\"] = False\n",
    "            result[\"reason\"] = \"Original does not imply generated.\"\n",
    "            return result\n",
    "\n",
    "        if results[1] != \"unsat\":\n",
    "            result[\"result\"] = False\n",
    "            result[\"reason\"] = \"Generated does not imply original.\"\n",
    "            return result\n",
    "\n",
    "        result[\"result\"] = True\n",
    "        result[\"reason\"] = \"Constraints are logically equivalent.\"\n",
    "    except Exception as e:\n",
    "        result[\"result\"] = False\n",
    "        result[\"reason\"] = f\"Error running Z3: {e}\"\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d62f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for batch in client.batches.list():\n",
    "    if batch.created_at >= 1746474126 and batch.output_file_id is not None:\n",
    "        client.files.content(batch.output_file_id).stream_to_file(f\"{client.files.retrieve(batch.input_file_id).filename}.{datetime.datetime.fromtimestamp(batch.created_at).strftime('%Y-%m-%d-%H:%M:%S')}\")\n",
    "        model = client.files.retrieve(batch.input_file_id).filename.rsplit(\"-\",maxsplit=1)[0]\n",
    "\n",
    "        # Load the batch response into a DataFrame\n",
    "        with open(f\"{client.files.retrieve(batch.input_file_id).filename}.{datetime.datetime.fromtimestamp(batch.created_at).strftime('%Y-%m-%d-%H:%M:%S')}\") as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Extract content from each response\n",
    "        df[\"content\"] = df[\"response\"].apply(\n",
    "            lambda r: (\n",
    "                r[\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "                if (r and \"body\" in r and \"choices\" in r[\"body\"] and len(r[\"body\"][\"choices\"]) > 0)\n",
    "                else None\n",
    "            )\n",
    "        )\n",
    "        df = df[[\"custom_id\", \"content\"]]\n",
    "        df[\"custom_id\"] = df[\"custom_id\"].astype(int)\n",
    "\n",
    "        # Merge with original dataset\n",
    "        merged_df = dataset.merge(df, left_index=True, right_on=\"custom_id\", how=\"left\")\n",
    "\n",
    "        # Evaluate each response\n",
    "        results = []\n",
    "        for i, row in merged_df.iterrows():\n",
    "            response = row[\"content\"]\n",
    "            truth = row[\"answer\"]\n",
    "            constants = row[\"constants\"]\n",
    "            question = row[\"question\"]\n",
    "\n",
    "            extracted = extract_response(response) if response else None\n",
    "            z3_result = (\n",
    "                evaluate_with_z3(response=extracted, truth=truth, constants=constants)\n",
    "                if extracted else {\"result\": False, \"reason\": \"Failed to extract response.\"}\n",
    "            )\n",
    "\n",
    "            results.append({\n",
    "                \"custom_id\": row[\"custom_id\"],\n",
    "                \"question\": question,\n",
    "                \"response\": response,\n",
    "                \"extracted\": extracted,\n",
    "                \"truth\": truth,\n",
    "                \"constants\": constants,\n",
    "                \"z3_result\": z3_result[\"result\"],\n",
    "                \"reason\": z3_result[\"reason\"]\n",
    "            })\n",
    "\n",
    "        # Compute final metrics and save\n",
    "        result_summary = {\n",
    "            \"model\": model,\n",
    "            \"accuracy\": sum(1 for r in results if r[\"z3_result\"]) / len(results) if len(results) else 0,\n",
    "            \"correct\": sum(1 for r in results if r[\"z3_result\"]),\n",
    "            \"incorrect\": sum(1 for r in results if not r[\"z3_result\"]),\n",
    "            \"results\": results,\n",
    "        }\n",
    "        with open(f\"{model}-{datetime.datetime.fromtimestamp(batch.created_at).strftime('%Y-%m-%d-%H:%M:%S')}.json\", \"w\") as f:\n",
    "            json.dump(result_summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebb4cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "def gather_and_save_accuracy_data(root_path, output_file):\n",
    "    \"\"\"\n",
    "    Scans subfolders named 'trialN' (with N being an integer) under 'root_path',\n",
    "    reads all .json files in each trial folder, and collects model accuracies.\n",
    "    For each model, computes the average accuracy per trial and an overall average.\n",
    "    The final structure has one entry per model with keys for each trial and an 'average'\n",
    "    entry summarizing overall performance. The resulting dictionary is saved as a JSON file.\n",
    "\n",
    "    Example output:\n",
    "    {\n",
    "        \"ModelA\": {\n",
    "            \"trials\": {\n",
    "                \"trial_1\": 0.85,\n",
    "                \"trial_2\": 0.88,\n",
    "                ...\n",
    "            },\n",
    "            \"average\": 0.86\n",
    "        },\n",
    "        \"ModelB\": {\n",
    "            \"trials\": {\n",
    "                \"trial_1\": 0.80,\n",
    "                \"trial_2\": 0.82,\n",
    "                ...\n",
    "            },\n",
    "            \"average\": 0.81\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    Parameters:\n",
    "    - root_path: str, path to the directory containing trial subfolders.\n",
    "    - output_file: str, path to the output JSON file.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with model names as keys and their corresponding trial and overall averages.\n",
    "    \"\"\"\n",
    "    model_results = {}\n",
    "    trial_pattern = re.compile(r\"trial(\\d+)\")\n",
    "    \n",
    "    # Loop over items in the root directory\n",
    "    for item in os.listdir(root_path):\n",
    "        match = trial_pattern.match(item)\n",
    "        if match:\n",
    "            trial_number = int(match.group(1))\n",
    "            trial_folder_path = os.path.join(root_path, item)\n",
    "            \n",
    "            if not os.path.isdir(trial_folder_path):\n",
    "                continue\n",
    "            \n",
    "            # Scan for JSON files in each trial folder\n",
    "            for filename in os.listdir(trial_folder_path):\n",
    "                if filename.endswith(\".json\"):\n",
    "                    file_path = os.path.join(trial_folder_path, filename)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            data = json.load(f)\n",
    "                        model_name = data.get(\"model\")\n",
    "                        accuracy = data.get(\"accuracy\")\n",
    "                        \n",
    "                        # Only add if both model name and accuracy are present\n",
    "                        if model_name is None or accuracy is None:\n",
    "                            continue\n",
    "                        \n",
    "                        if model_name not in model_results:\n",
    "                            model_results[model_name] = {}\n",
    "                        \n",
    "                        if trial_number not in model_results[model_name]:\n",
    "                            model_results[model_name][trial_number] = []\n",
    "                        \n",
    "                        model_results[model_name][trial_number].append(accuracy)\n",
    "                    except json.JSONDecodeError:\n",
    "                        print(f\"Warning: Could not parse JSON from {file_path}\")\n",
    "    \n",
    "    # Compute average accuracies per trial and overall for each model\n",
    "    output_dict = {}\n",
    "    for model, trials in model_results.items():\n",
    "        trial_averages = {}\n",
    "        overall_total = 0.0\n",
    "        overall_count = 0\n",
    "        \n",
    "        for trial_no in sorted(trials.keys()):\n",
    "            accuracies = trials[trial_no]\n",
    "            avg_trial = sum(accuracies) / len(accuracies)\n",
    "            trial_averages[f\"trial_{trial_no}\"] = avg_trial\n",
    "            overall_total += sum(accuracies)\n",
    "            overall_count += len(accuracies)\n",
    "        \n",
    "        overall_average = overall_total / overall_count if overall_count > 0 else None\n",
    "        output_dict[model] = {\"trials\": trial_averages, \"average\": overall_average}\n",
    "    \n",
    "    # Save the resulting dictionary to a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_f:\n",
    "        json.dump(output_dict, out_f, indent=4)\n",
    "    \n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69787d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_and_save_accuracy_data(\n",
    "    root_path=\"./results\",\n",
    "    output_file=\"accuracy_summary.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
